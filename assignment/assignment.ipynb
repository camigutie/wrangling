{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  Cleaning data is a critical step in analysis though there has been few advancements in the process to make it more effective. Data tidying is the concept of organizing a dataset so that each variable is a column, each observation is a row, and each type of observational unit is a table. This paper focuses on a framework that takes advantage of matching tools and data structures.\n",
        " 2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "\n",
        "\n",
        " Data tidying is the structuring of datasets to facilitate analysis and is estimated to be 80% of a data scientist's role. The tidy data standard is intended to provide a default way to organize data within a dataset, making it a less time consuming step. The framework that is provided in this paper provides a “philisophy of data” that is explained in the plyr and ggplot2 packages.\n",
        "  \n",
        " 3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "\n",
        "\n",
        "The sentence means that organized data has a common set of properties whether a physical layout or grouping that make similar while messy datasets each have unique challenges making them complex to analyze. The next sentence regarding observations and variables refers to the idea that the there are many wats to organize the same underlying data. Formulating a broader more-encompassing definition for variables and observations is difficult due to the importance of context in these decisions.\n",
        "   \n",
        " 4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "\n",
        "\n",
        "Wickman defines values as what datasets are composed of as either numbers or strings. These values measuring the same attribute (ex: height, temperature, duration) are then assigned to a variable or an observation with measurement on the same unit (ex: person, or a day, or a race).\n",
        "\n",
        " 5. How is \"Tidy Data\" defined in section 2.3?\n",
        "\n",
        "\n",
        "Tidy data is a defined as when each variable forms a column, each observation forms a row, and each type of observational unity forms a table. The alternative is messy data where it is arranged in any other fashion.\n",
        "\n",
        "\n",
        " 6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "\n",
        "\n",
        "There are 5 common problems with messy datasets: column headers are values, not variable names, multiple variables are stored in one column, variables are stored in both rows and columns, multiple types of observational units are stored in the same table, and a single observational unit is stored in multiple tables. The data in Table 4 are messy since the column headers are values and not variable name. They are values for a variable that is income which should be next to the column for the religion variable so that each variable forms a column. Melting is turning columns into rows to create a molten dataset.  \n",
        "\n",
        "\n",
        " 7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "\n",
        "\n",
        "Table 11 is messy sincethere is a column for each day of the month, the variables are stored in both rows and columns. The element column is not a\n",
        "variable; it stores the names of variables. Table 12(a) is “molten” since the tmax and tmin variable are in rows and need to be unstacked to the tidy weather dataset where by rotating the element variable back out into the columns.\n",
        "\n",
        "\n",
        " 8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "\n",
        "\n",
        "The “chicken-and-egg” problem is that if tidy data is only as useful as the tools that work with it, then tidy tools will be inextricably linked to tidy data. Wickman acknowledges that he does not see the tidy data framework as the final solution and hopes that other will build on the framework to develop better storage strategies and better tools. He likewise hopes that more frameworks are developed for the other jobs involved in cleaning data such as parsing dates and numbers, identifying missing values, and correcting character encodings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "! git clone https://github.com/DS3001/wrangling"
      ],
      "metadata": {
        "id": "NgeYVeqISDty",
        "outputId": "4110a23b-40a2-4778-baeb-5300dc3b6eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NgeYVeqISDty",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'wrangling' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('wrangling/assignment/data/airbnb_hw.csv', low_memory=False)\n",
        "print( df.shape, '\\n')\n",
        "df.head()\n",
        "\n",
        "price = df['Price']\n",
        "price.unique() #see that there are comma seperators for numbers over 1000\n",
        "\n",
        "price = price.str.replace(',','') #Remove separator commas\n",
        "price = pd.to_numeric(price,errors='coerce') #Coerce the values to numeric using the Pandas method\n",
        "\n",
        "print(\"Missing Values:\", sum(price.isnull())) #sum of the missing values\n"
      ],
      "metadata": {
        "id": "L4Wm_BqYSRW2",
        "outputId": "3c1a4c2d-75de-4578-bd9a-876362672b8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "L4Wm_BqYSRW2",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30478, 13) \n",
            "\n",
            "Missing Values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make."
      ],
      "metadata": {
        "id": "A1EIiUwhXtMk"
      },
      "id": "A1EIiUwhXtMk"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('wrangling/assignment/data/sharks.csv', low_memory=False)\n",
        "df['Type'].value_counts()"
      ],
      "metadata": {
        "id": "WFOpWDpHXxrn",
        "outputId": "9b6d9979-1bea-411c-bf6f-9b937ea1e224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WFOpWDpHXxrn",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unprovoked             4716\n",
              "Provoked                593\n",
              "Invalid                 552\n",
              "Sea Disaster            239\n",
              "Watercraft              142\n",
              "Boat                    109\n",
              "Boating                  92\n",
              "Questionable             10\n",
              "Unconfirmed               1\n",
              "Unverified                1\n",
              "Under investigation       1\n",
              "Boatomg                   1\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type = df['Type'] # Create a temporary vector of values for the Type variable\n",
        "\n",
        "type = type.replace(['Sea Disaster', 'Boat','Boating','Boatomg'],'Watercraft') # There is duplicates of watercraft/boating values\n",
        "type = type.replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation'],np.nan) # Treat as missing variables with np.nan from the NumPy package: \"Not-a-number\", and its type is float\n",
        "type.value_counts()\n",
        "\n",
        "df['Type'] = type # Replace the 'Type' variable with the cleaned dataset\n",
        "del type # Delete the temporary vector\n",
        "\n",
        "df['Type'].value_counts()"
      ],
      "metadata": {
        "id": "s7pZNdCbnWpt",
        "outputId": "6469220a-3136-4884-ffca-9465c9f9392f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s7pZNdCbnWpt",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unprovoked    4716\n",
              "Provoked       593\n",
              "Watercraft     583\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`."
      ],
      "metadata": {
        "id": "JmsqhcXhouuv"
      },
      "id": "JmsqhcXhouuv"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv', low_memory=False)\n",
        "print(df['WhetherDefendantWasReleasedPretrial'].value_counts(),'\\n')\n",
        "\n",
        "release = df['WhetherDefendantWasReleasedPretrial'] #Create a temporary vector of values for the variable\n",
        "print(release.unique(),'\\n')\n",
        "release = release.replace(9,np.nan) # Based off the codebook, we know 9's are \"unclear\"\n",
        "print(release.value_counts(),'\\n')\n",
        "sum(release.isnull()) # we see there are 31 missing values as 9's\n",
        "df['WhetherDefendantWasReleasedPretrial'] = release # Replace variable with the cleaned values\n",
        "del release # Delete the temporary vector"
      ],
      "metadata": {
        "id": "fozyLOSNoypg",
        "outputId": "410a01ba-3552-49b4-a30e-9dcccb7d8875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fozyLOSNoypg",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: WhetherDefendantWasReleasedPretrial, dtype: int64 \n",
            "\n",
            "[9 0 1] \n",
            "\n",
            "1.0    19154\n",
            "0.0     3801\n",
            "Name: WhetherDefendantWasReleasedPretrial, dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)\n"
      ],
      "metadata": {
        "id": "Fzw52xb4qy0M"
      },
      "id": "Fzw52xb4qy0M"
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = df['ImposedSentenceAllChargeInContactEvent']\n",
        "type = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "\n",
        "sentence = pd.to_numeric(sentence,errors='coerce') #Coerce the values to numeric using the Pandas method\n",
        "sentence_NA = sentence.isnull() # Create a missing dummy\n",
        "print( np.sum(sentence_NA),'\\n') # see we have many missing values (9k of 23k)\n",
        "\n",
        "print( pd.crosstab(sentence_NA, type), '\\n') # group 4 is cases with charges were dismissed/pending/deferred\n",
        "\n",
        "sentence = sentence.mask( type == 4, 0) # Sentence is 0 when type ==4\n",
        "sentence = sentence.mask( type == 9, np.nan) # Sentence is np.nan when type == 9 since group 9 has no sentencing record\n",
        "\n",
        "sentence_NA = sentence.isnull()\n",
        "print( np.sum(sentence_NA),'\\n') # see only 274 missing value\n",
        "\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = sentence # Replace variable with the cleaned values\n",
        "del sentence, type # Delete the temporary vector"
      ],
      "metadata": {
        "id": "l-rypCj-q9GV",
        "outputId": "86fd9953-0248-4d7a-8c66-52212d5583ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l-rypCj-q9GV",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "274 \n",
            "\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914  8779    0\n",
            "True                                                 0     0    0     0  274 \n",
            "\n",
            "274 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gURvpKadqyhS"
      },
      "id": "gURvpKadqyhS"
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}